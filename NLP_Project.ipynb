{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nxy5n8FVUYTX"
   },
   "source": [
    "# Sentiment classification\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MAhLFwkx_HBA"
   },
   "outputs": [],
   "source": [
    "#Importing the dataset in pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('/content/Reviews.csv', sep = ',', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6CvEziqjB8CM",
    "outputId": "1cde0c59-bf29-4047-baf5-44ae546f5cc3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568454, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "colab_type": "code",
    "id": "AORM1QKPA3p1",
    "outputId": "6a59d0b2-0d9f-46e4-b456-6621af52817c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  ...                                               Text\n",
       "0   1  ...  I have bought several of the Vitality canned d...\n",
       "1   2  ...  Product arrived labeled as Jumbo Salted Peanut...\n",
       "2   3  ...  This is a confection that has been around a fe...\n",
       "3   4  ...  If you are looking for the secret ingredient i...\n",
       "4   5  ...  Great taffy at a great price.  There was a wid...\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "86Kzc7CvEznC"
   },
   "outputs": [],
   "source": [
    "#Task is to classify sentiments of text so keeping only the text data and rating for classification\n",
    "data = data[['Text', 'Score']].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ukds2TLIFZVu"
   },
   "source": [
    "Amazon Food Review uses five-star rating system. Counting the reviews by their score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "colab_type": "code",
    "id": "6zD-8-s1FXM8",
    "outputId": "da6ed154-6b6d-41da-f35c-1e32c218096a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "score_count = data['Score'].value_counts()\n",
    "plot = score_count.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9AgEvGIALa9P"
   },
   "source": [
    "There are much more reviews with rating 5. To avoid using a biased dataset and to balanced it, considering rating 4 and 5 as positive class and 1, 2, and 3 as negative class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "KVXNmrxljyYN",
    "outputId": "c00ed65f-eaec-4f87-9c39-96cdbd7b8b54"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data['Score'][data['Score'] <= 3]= str('Neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u7iOaksjqPL3"
   },
   "outputs": [],
   "source": [
    "data['Score'][data['Score'] != 'Neg']= str('Pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "unpH7VwXxEev",
    "outputId": "fb7f7158-29a1-4608-fffa-b69d989d09d2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>Pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>Neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>Pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>Neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>Pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Score\n",
       "0  I have bought several of the Vitality canned d...   Pos\n",
       "1  Product arrived labeled as Jumbo Salted Peanut...   Neg\n",
       "2  This is a confection that has been around a fe...   Pos\n",
       "3  If you are looking for the secret ingredient i...   Neg\n",
       "4  Great taffy at a great price.  There was a wid...   Pos"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checing the columns in dataset\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sdJALzCTxJ-G"
   },
   "outputs": [],
   "source": [
    "#separating positive and negative reviews \n",
    "data_pos = data[data['Score'] == 'Pos']\n",
    "data_neg = data[data['Score'] == 'Neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Le1hctZE5Gfp",
    "outputId": "6409cad6-30c0-4364-91a6-15801894d51a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124677, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_neg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cWcLPNDK3PDT",
    "outputId": "016d0529-f73f-462c-ba80-4e2ae08a6fa2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(443777, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tm2urDiIVvc0"
   },
   "source": [
    "Positive review dataset has more than 2 times the reviews in negative dataset. To avoid bias, reducing the number of datapoints in positive dataset equal to the datapoints in negative dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_JKlcpNh5W6l"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "#shuffling data to select reviews at random\n",
    "data_pos = shuffle(data_pos)\n",
    "data_pos = data_pos[:len(data_neg)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "X6wTgO39GVYn",
    "outputId": "e0d2d7d8-2ed3-42b2-89c3-4b5b7c3a2e03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124677, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FFc6XdUjGhPr"
   },
   "outputs": [],
   "source": [
    "#Splitting positive class into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "data_pos_train, data_pos_test = train_test_split(data_pos, test_size = 0.3, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IV6cV3pAH2Se"
   },
   "outputs": [],
   "source": [
    "#Splitting negative class into train and test set\n",
    "data_neg_train, data_neg_test = train_test_split(data_neg, test_size = 0.3, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iFZ6wvN6M_ks"
   },
   "outputs": [],
   "source": [
    "#Creating training dataframe\n",
    "traindf = pd.concat([data_pos_train, data_neg_train])\n",
    "traindf = shuffle(traindf)\n",
    "traindf = traindf.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XvzTK-pyPOuI"
   },
   "outputs": [],
   "source": [
    "#creating testing dataframe\n",
    "testdf = pd.concat([data_pos_test, data_neg_test])\n",
    "testdf = shuffle(testdf)\n",
    "testdf = testdf.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qkHTbveOf8DZ"
   },
   "source": [
    "# 1. Tf-idf + Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oBYn7biJWhTr"
   },
   "source": [
    "Importing necessary libraries for TfIdf algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "OFFN4xPOIfvf",
    "outputId": "81999946-0aed-4d3e-d11c-5beb5c20531f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer,HashingVectorizer\n",
    "#from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "import string\n",
    "#import re\n",
    "import spacy\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "spacy.load('en')\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZPJTIXAcWq8o"
   },
   "source": [
    "Defining methods to be used for cleaning the text to be used for classification. Removing stopwords and punctuations. Tokenizing and Lemmatizing the words in the text to bring words having different forms but same meaning to their base forms. This is important as it helps reduce redundancy of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YbIrCbUyJIaR"
   },
   "outputs": [],
   "source": [
    "#Stop words and spcecial characters \n",
    "StopWords = set(stopwords.words('english') + list(ENGLISH_STOP_WORDS)) \n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-\", \"...\", \"”\", \"”\",\"''\"]\n",
    "\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenizer(text):\n",
    "  text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "  text = text.lower()\n",
    "  tokens = parser(text)\n",
    "  lemmas = []\n",
    "  for tok in tokens:\n",
    "      lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
    "  tokens = lemmas\n",
    "  tokens = nltk.tokenize.word_tokenize(text)\n",
    "  tokens = [tok for tok in tokens if len(tok) > 2]\n",
    "  #tokens = [lemmatizer.lemmatize(tok) for tok in tokens]\n",
    "  tokens = [tok for tok in tokens if tok not in StopWords]\n",
    "  tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
    "  tokens = list(set(tokens))\n",
    "\n",
    "  return ' '.join(tokens[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "08MzW4LGbZq6"
   },
   "outputs": [],
   "source": [
    "traindf['Text'] = traindf['Text'].apply(lambda x:tokenizer(x))\n",
    "testdf['Text'] = testdf['Text'].apply(lambda x:tokenizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v6UXu7gIeqt5"
   },
   "outputs": [],
   "source": [
    "y_train = traindf['Score']\n",
    "x_train = traindf['Text']\n",
    "\n",
    "y_test = testdf['Score']\n",
    "x_test = testdf['Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sfx-ezlXXqgA"
   },
   "source": [
    "TfidfTransformer uses CountVentorizer to count the number of words (term frequency). Training tfidf and evaluating the model on unseen data. TfidfVectorizer also works similar to tfidfTransformer although, it does certain operations all at once. The main difference between the two is that TfidfTransformer will perform step-by-step task of calculating the word count with CountVextorizer, IDF values and Tf-idf scores. Whereas, Tfidfvectorizer will do all these steps in 1 step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DPYvLc5VgjR2"
   },
   "outputs": [],
   "source": [
    "#tfidftransformer - CountVectorizer\n",
    "count_vect = CountVectorizer(analyzer = 'word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(x_train)\n",
    "xtrain_count = count_vect.transform(x_train)\n",
    "xtest_count = count_vect.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xHQoSDt7_9V_"
   },
   "outputs": [],
   "source": [
    "#tfidfvectorizer\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(x_train)\n",
    "xtrain_tfidf = tfidf_vect.transform(x_train)\n",
    "xtest_tfidf = tfidf_vect.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FIMQJp3Td0wc"
   },
   "source": [
    "Using Tf-idf model with classical statistical model, Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVwhNfCtFlZN"
   },
   "outputs": [],
   "source": [
    "def training_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    #Fitting the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    #Predicting the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "LgxJzfVnFoWM",
    "outputId": "68b50b26-51aa-4571-cb2b-cc8f8bb969cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF accuracy score, Count Vectors:  0.8268768046198267\n",
      "RF accuracy score, WordLevel TF-IDF:  0.8361271521762378\n"
     ]
    }
   ],
   "source": [
    "from sklearn import decomposition, ensemble\n",
    "from sklearn import model_selection, preprocessing, metrics\n",
    "#RF on Count Vectors\n",
    "accuracy = training_model(ensemble.RandomForestClassifier(n_estimators=8), xtrain_count, y_train, xtest_count)\n",
    "\n",
    "print(\"RF accuracy score, Count Vectors: \", accuracy)\n",
    "\n",
    "#RF on Word Level TF IDF Vectors\n",
    "accuracy = training_model(ensemble.RandomForestClassifier(n_estimators=8), xtrain_tfidf, y_train, xtest_tfidf)\n",
    "print(\"RF accuracy score, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7oU6WDOod9na"
   },
   "source": [
    "The accuracy score are very close to each other for tfidfVectorizer and tfidftransformer. If computation of tfidf needs to be done on documents other than the training dataset, either of the two modules can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AXqYwa65HBfI"
   },
   "source": [
    "# 2. LSTM Text Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "98Bfe47zvpwz",
    "outputId": "f7f872aa-ce40-4246-df54-326b51b20f7b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ubBlyiS-HMY8"
   },
   "outputs": [],
   "source": [
    "#Using the positive and negative datasets together\n",
    "\n",
    "review_text = pd.concat([data_pos['Text'], data_neg['Text']])\n",
    "review_label = pd.concat([data_pos['Score'], data_neg['Score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "JF4NE882pS5o",
    "outputId": "4132fc1f-cd08-49de-9c40-f8ecb2f25547"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(249354, 250)"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenizing the dataset\n",
    "#Max no. of words to use\n",
    "max_words = 50000\n",
    "#Max no. of words in each review\n",
    "max_features = 250\n",
    "#defining tokenizer\n",
    "review_text = shuffle(review_text)\n",
    "token = Tokenizer(num_words=max_words, split = ' ', filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower = True)\n",
    "token.fit_on_texts(review_text)\n",
    "word_index = token.word_index\n",
    "print(len(word_index))\n",
    "Int = token.texts_to_sequences(review_text.values)\n",
    "Int = sequence.pad_sequences(Int, maxlen=max_features)\n",
    "Int.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "l_7EMCuIYZB4",
    "outputId": "8d97f896-7e1d-4d94-a943-031a15396cae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(249354, 2)"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_label = shuffle(review_label)\n",
    "Out = pd.get_dummies(review_label).values\n",
    "Out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6R8hPfVdwagd"
   },
   "outputs": [],
   "source": [
    "#splitting data\n",
    "Int_train, Int_test, Out_train, Out_test = train_test_split(Int, Out, test_size=0.2, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9_B-MxB-yNo-",
    "outputId": "02cd11d5-e072-48e8-e7a5-2b3157af4a80"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.framework.ops.get_default_graph>"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hyperparameters\n",
    "batch_size = 64\n",
    "embedding_dims = 100\n",
    "lstm = 128\n",
    "input_length = Int.shape[1]\n",
    "\n",
    "tf.compat.v1.get_default_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "id": "aG3mRr1lybw3",
    "outputId": "527810a6-a124-443f-edb7-d9135c5ce3f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 250, 100)          5000000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 5,117,506\n",
      "Trainable params: 5,117,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#LSTM model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dims, input_length=input_length))\n",
    "model.add(LSTM(lstm, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2 ,activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YYWxJMnXyozR"
   },
   "outputs": [],
   "source": [
    "# Loss\n",
    "loss = 'binary_crossentropy' \n",
    "# Optimizer\n",
    "optimizer = 'adam' \n",
    "# Compilation\n",
    "#############\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "Oy1c5QOQyqKC",
    "outputId": "51903cd8-9385-4544-ce74-4970581c3fe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2806/2806 [==============================] - 2588s 922ms/step - loss: 0.6933 - accuracy: 0.5006 - val_loss: 0.6932 - val_accuracy: 0.4970\n",
      "Epoch 2/5\n",
      "2806/2806 [==============================] - 2570s 916ms/step - loss: 0.6909 - accuracy: 0.5283 - val_loss: 0.6958 - val_accuracy: 0.4978\n",
      "Epoch 3/5\n",
      "2806/2806 [==============================] - 2562s 913ms/step - loss: 0.6788 - accuracy: 0.5635 - val_loss: 0.7085 - val_accuracy: 0.4931\n",
      "Epoch 4/5\n",
      "2806/2806 [==============================] - 2645s 942ms/step - loss: 0.6530 - accuracy: 0.6016 - val_loss: 0.7350 - val_accuracy: 0.4958\n",
      "Epoch 5/5\n",
      "2806/2806 [==============================] - 2660s 948ms/step - loss: 0.6200 - accuracy: 0.6347 - val_loss: 0.7644 - val_accuracy: 0.4953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe465fe2278>"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training model\n",
    "model.fit(Int_train, Out_train, epochs=5, batch_size=batch_size, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Micg2lUfy1TT"
   },
   "source": [
    "val_loss is increasing with each incrementing epoch. The number of learning parameters can be reduced here as because of too many learning parameters, model is learning the examples and so not working well with test/validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "s5-yLTEF1Uap",
    "outputId": "490df1ad-8c89-40e9-a5f5-708f685d3364"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.4966012239456177\n"
     ]
    }
   ],
   "source": [
    "test_score = model.evaluate(Int_test, Out_test, verbose=0)\n",
    "print('Test accuracy:', test_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iLZuWOPi0yc7"
   },
   "source": [
    "There are less number of examples in the test dataset. The model performs poorly on test data as compared to training data. The model has high variance and seems to be overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b9DmZfyhc1Wu"
   },
   "source": [
    "# 3. LSTM Model: where the embeddings are initialized with pre-trained GloVe vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ns6a7IJlMIrM"
   },
   "outputs": [],
   "source": [
    "#Creating directory for storing pre-processed data\n",
    "if(not os.path.isdir('preprocessed_data')):\n",
    "    os.mkdir('preprocessed_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PewORY7NxWiB"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "from keras.initializers import Constant\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "7SDqjdtLxkuK",
    "outputId": "901448e6-acda-4af2-d278-41a01449cfcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-30 15:24:26--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2020-05-30 15:24:26--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2020-05-30 15:24:27--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip.1’\n",
      "\n",
      "glove.6B.zip.1      100%[===================>] 822.24M  1.95MB/s    in 6m 27s  \n",
      "\n",
      "2020-05-30 15:30:54 (2.12 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
      "\n",
      "Archive:  glove.6B.zip\n",
      "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n",
      "glove.6B.100d.txt  glove.6B.50d.txt  preprocessed_data\n",
      "glove.6B.200d.txt  glove.6B.zip      Reviews.csv\n",
      "glove.6B.300d.txt  glove.6B.zip.1    sample_data\n",
      "/content\n"
     ]
    }
   ],
   "source": [
    "#Downloading pre-trained GloVe Vectors\n",
    "\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip\n",
    "!ls\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PebirGLl0tE2"
   },
   "outputs": [],
   "source": [
    "base_dir = '/content/'\n",
    "glove_dir = os.path.join(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8rCKkznj1ufW",
    "outputId": "c997d60a-a19a-4655-8900-2ced5f0e9377"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#Building the index mapping words in the embeddings set to their respective embedding vector\n",
    "\n",
    "embedding_index = {}\n",
    "with open(os.path.join(glove_dir, 'glove.6B.300d.txt')) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embedding_index))\n",
    "#print(len(embedding_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nm4eYsJ6P5jo"
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 50 #max length of each sentence\n",
    "MAX_WORDS = 150000 #max no. of words for tokenizer\n",
    "EMBEDDING_DIM = 300 #wmbedding dimensions for word vectors (GloVe)\n",
    "LSTM_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BF665IzF1ymW"
   },
   "outputs": [],
   "source": [
    "#Tokenizing the dataset\n",
    "max_features = 100\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ', filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(review_text)\n",
    "X_gl = tokenizer.texts_to_sequences(review_text.values)\n",
    "X_gl = sequence.pad_sequences(X_gl, maxlen=max_features)\n",
    "review_label = shuffle(review_label)\n",
    "Y_gl = pd.get_dummies(review_label).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "1SPzvJhU13b2",
    "outputId": "6a3b7692-c51a-47bf-d589-a7918040c517"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 93000 unique tokens.\n",
      "Data tensor shape: (249354, 50)\n",
      "Label tensor shape: (249354, 2)\n"
     ]
    }
   ],
   "source": [
    "#Vectorizing the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(review_text)\n",
    "sequences = tokenizer.texts_to_sequences(review_text.values)\n",
    "\n",
    "word_idx = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_idx))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQ_LENGTH)\n",
    "\n",
    "#labels = to_categorical(np.asarray(review_label))\n",
    "labels = np.asarray(pd.get_dummies(review_label))\n",
    "print('Data tensor shape:', data.shape)\n",
    "print('Label tensor shape:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "80iXNd8V0w2k"
   },
   "outputs": [],
   "source": [
    "#creating a weight matrix\n",
    "num_words = min(MAX_WORDS, len(word_idx) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_idx.items():\n",
    "    if i >= MAX_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fS5WxI3NKEi2"
   },
   "outputs": [],
   "source": [
    "# Data split into training  and test set \n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(0.2 * data.shape[0])\n",
    "\n",
    "x_train_gl = data[:-num_validation_samples]\n",
    "y_train_gl = labels[:-num_validation_samples]\n",
    "x_test_gl = data[-num_validation_samples:]\n",
    "y_test_gl = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DHEI-a-5Rs2Y"
   },
   "outputs": [],
   "source": [
    "#Embedding Layer\n",
    "embedding_layer = Embedding(input_dim=len(word_idx) + 1,\n",
    "                                output_dim=EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQ_LENGTH,\n",
    "                                trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "xPMn_V_uR2gL",
    "outputId": "8621df21-910c-42c8-e4d2-c2cd2b86292b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 300)           27900300  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 28,622,102\n",
      "Trainable params: 28,622,102\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(LSTM_SIZE, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w0y7-za1R5BS"
   },
   "outputs": [],
   "source": [
    "#Loss\n",
    "loss = 'binary_crossentropy' \n",
    "#Optimizer\n",
    "optimizer = 'nadam' \n",
    "#Model Compilation\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "gy48-QP6R7eb",
    "outputId": "02b4b494-ed9e-499b-bcc7-6b070c6d00dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159587 samples, validate on 39897 samples\n",
      "Epoch 1/5\n",
      "159587/159587 [==============================] - 391s 2ms/sample - loss: 0.6953 - accuracy: 0.5007 - val_loss: 0.6936 - val_accuracy: 0.5006\n",
      "Epoch 2/5\n",
      "159587/159587 [==============================] - 385s 2ms/sample - loss: 0.6916 - accuracy: 0.5179 - val_loss: 0.6948 - val_accuracy: 0.5036\n",
      "Epoch 3/5\n",
      "159587/159587 [==============================] - 386s 2ms/sample - loss: 0.6828 - accuracy: 0.5477 - val_loss: 0.6999 - val_accuracy: 0.5011\n",
      "Epoch 4/5\n",
      "159587/159587 [==============================] - 384s 2ms/sample - loss: 0.6628 - accuracy: 0.5804 - val_loss: 0.7183 - val_accuracy: 0.4987\n",
      "Epoch 5/5\n",
      "159587/159587 [==============================] - 383s 2ms/sample - loss: 0.6347 - accuracy: 0.6144 - val_loss: 0.7519 - val_accuracy: 0.5010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c04312550>"
      ]
     },
     "execution_count": 64,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting model\n",
    "model.fit(x_train_gl, y_train_gl,validation_split=0.2,batch_size=120,epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-4aumApE33Ii"
   },
   "source": [
    "There is a slight improvement in model performance with pretrained GloVe vectors with introduction of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "obI5aVUzSEu5",
    "outputId": "5da9b4af-b2d4-465f-e36d-c9c35acaffb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.50382996\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test_gl, y_test_gl, verbose=0)\n",
    "print('Test Accuracy:' , scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jnmSCE0D3zZy"
   },
   "source": [
    "There are less number of examples in the test dataset. The model performs poorly on test data as compared to training data. The model has high variance and seems to be overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HXjcuN9-SbPt"
   },
   "source": [
    "# 4. fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QUu3X894DYqU"
   },
   "outputs": [],
   "source": [
    "!pip install ktrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8DEZUJ9qDdFc"
   },
   "outputs": [],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ng4AeNMFDgB8"
   },
   "outputs": [],
   "source": [
    "import ktrain\n",
    "from ktrain import text\n",
    "#%tensorflow_version 2.x\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iWXf_0fUaCdy"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "df = pd.read_csv('/content/Reviews.csv', sep = ',', engine='python')\n",
    "df = shuffle(df)\n",
    "df = df[:75000]\n",
    "df = df[['Text', 'Score']]\n",
    "df['Score'] = df['Score'].apply(lambda x: 'neg' if x <= 3 else 'pos')\n",
    "df.columns = ['text', 'label']\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Br7-GV_q-gan"
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df, df.label.astype('str').str.get_dummies()], axis=1, sort=False)\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UxbwOZvp-pqA"
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns='label')\n",
    "df = df[['text', 'neg', 'pos']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OezPyfGgE3L2"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test), preproc = text.texts_from_df(df, 'text', label_columns=['neg', 'pos'],\n",
    "                                                                  maxlen=75, max_features=100000,\n",
    "                                                                  preprocess_mode='standard', val_pct=0.1,\n",
    "                                                                  ngram_range=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XQze2oAscX7f"
   },
   "outputs": [],
   "source": [
    "text.print_text_classifiers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mrTWX1RGcczY"
   },
   "outputs": [],
   "source": [
    "model = text.text_classifier('fasttext', (x_train, y_train), preproc=preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6o9opbGvcjAW"
   },
   "outputs": [],
   "source": [
    "learner_mod = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WNOPuu-DczHx"
   },
   "outputs": [],
   "source": [
    "#Learning rate finder\n",
    "learner_mod.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-uKzjPxXc089"
   },
   "outputs": [],
   "source": [
    "#Learning rate plot\n",
    "learner_mod.lr_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kW2ILsJcc2yl"
   },
   "outputs": [],
   "source": [
    "learner_mod.autofit(0.0009, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XujMix8nc5Xs"
   },
   "outputs": [],
   "source": [
    "predicting = ktrain.get_predictor(learner_mod.model, preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZILVpsy0c8Nv"
   },
   "outputs": [],
   "source": [
    "data = [ 'It was a boring movie! What a waste of money.',\n",
    "         'What an amazing piece of art. I would see the movie again.',\n",
    "        'What a beautiful romantic comedy.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KkH5WUh2c-vN"
   },
   "outputs": [],
   "source": [
    "predicting.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sVj09Bl6lP5t"
   },
   "source": [
    "# 5. BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OeSO3pdFlY5C"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test), preproc = text.texts_from_df(df, 'text', label_columns=['neg', 'pos'],\n",
    "                                                                      maxlen = 550,\n",
    "                                                                      preprocess_mode='bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0qg2nsDjmE1a"
   },
   "outputs": [],
   "source": [
    "learner_ktrain = ktrain.get_learner(text.text_classifier('bert',(x_train,y_train),preproc=preproc),\n",
    "                                                train_data=(x_train,y_train),\n",
    "                                                val_data=(x_test, y_test),\n",
    "                                                batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mr9ogsacmILy"
   },
   "outputs": [],
   "source": [
    "learner_ktrain.lr_find()\n",
    "learner_ktrain.lr_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sfjskl9xmKNY"
   },
   "outputs": [],
   "source": [
    "learner_ktrain.fit_onecycle(2e-5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l_bVroWufomi"
   },
   "outputs": [],
   "source": [
    "predicting = ktrain.get_predictor(learner_mod.model, preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2AJznJWfslu"
   },
   "outputs": [],
   "source": [
    "data = [ 'It was a boring movie! What a waste of money.',\n",
    "         'What an amazing piece of art. I would see the movie again.',\n",
    "        'What a beautiful romantic comedy.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1j-0gZbTfw5k"
   },
   "outputs": [],
   "source": [
    "predicting.predict(data)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_retake_working.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
