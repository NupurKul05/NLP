{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_retake_working.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxy5n8FVUYTX",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment classification - close to the state of the art\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5znBHuxhT34c",
        "colab_type": "text"
      },
      "source": [
        "The task of classifying sentiments of texts (for example movie or product reviews) has high practical significance in online marketing as well as financial prediction. This is a non-trivial task, since the concept of sentiment is not easily captured.\n",
        "For this assignment you have to use the Amazon Food Review (https://www.kaggle.com/snap/amazon-fine-food-reviews) dataset.\n",
        "The task is to try out multiple models in ascending complexity, namely:\n",
        "1.\tTFIDF + classical statistical model (eg. RandomForest)\n",
        "2.\tLSTM classification model\n",
        "3.\tLSTM model, where the embeddings are initialized with pre-trained GloVe vectors\n",
        "4.\tfastText model\n",
        "5.\tBERT based model (you are advised to use a pre-trained one and finetune, since the resource consumption is considerable!)\n",
        "You are allowed to use any library or tool, though the Keras environment, and some wrappers on top (ie. Ktrain) make your life easier. The performance metrics are exactly those given in the Kaggle contest.\n",
        "\n",
        "Groups This assignment is to be completed individually, two weeks after the class has finished. For the precise deadline please see canvas.\n",
        "Format of submission You need to submit a pdf of your Google Collab notebooks.\n",
        "Due date 30th of May.\n",
        "Grade distribution:\n",
        "1.\tTFIDF + classical statistical model (eg. RandomForest) (25% of the final grade)\n",
        "2.\tLSTM classification model (15% of the final grade)\n",
        "3.\tLSTM model, where the embeddings are initialized with pre-trained GloVe vectors (15% of the final grade)\n",
        "4.\tfastText model (15% of the final grade)\n",
        "5.\tBERT based model (you are advised to use a pre-trained one and finetune it, since the resource consumption is considerable!) (30% of the final grade). \n",
        "\n",
        "For each of the models, the marks will be awarded according to the following three criteria:\n",
        "(1) The (appropriately measured) accuracy of your prediction for the task. The more accurate the prediction is, the better. Note that you need to validate the predictive accuracy of your model on a hold-out of unseen data that the model has not been trained with.\n",
        "(2) How well you motivate the use of the model - what in this model's structure makes it suited for representing sentiment? After using the model for the task how well you evaluate the accuracy you got for each model and discuss the main advantages and disadvantages the model has in the particular modelling task. At best you take part of the modelling to support your arguments.\n",
        "(3) The consistency of your take-aways, i.e. what you have learned from your analyses. Also, analyze when the model is good and when and where it does not predict well.\n",
        "Please make sure that you comment with # on the separates steps of the code you have produced. For the verbal description and analyses plesae insert markdown cells.\n",
        "Plagiarism: The Frankfurt School does not accept any plagiarism. Data science is a collaborative exercise and you can discuss the research question with your classmates from other groups, if you like. You must not copy any code or text though. Plagiarism will be prosecuted and will result in a mark of 0 and you failing this class.\n",
        "Please submit the original Jupyter notebooks as well as a pdf of them. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAhLFwkx_HBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing the dataset in pandas dataframe\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('/content/Reviews.csv', sep = ',', engine='python')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CvEziqjB8CM",
        "colab_type": "code",
        "outputId": "1cde0c59-bf29-4047-baf5-44ae546f5cc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(568454, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AORM1QKPA3p1",
        "colab_type": "code",
        "outputId": "6a59d0b2-0d9f-46e4-b456-6621af52817c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "source": [
        "data.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>B001E4KFG0</td>\n",
              "      <td>A3SGXH7AUHU8GW</td>\n",
              "      <td>delmartian</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1303862400</td>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>B00813GRG4</td>\n",
              "      <td>A1D87F6ZCVE5NK</td>\n",
              "      <td>dll pa</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1346976000</td>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>B000LQOCH0</td>\n",
              "      <td>ABXLMWJIXXAIN</td>\n",
              "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1219017600</td>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>B000UA0QIQ</td>\n",
              "      <td>A395BORC6FGVXV</td>\n",
              "      <td>Karl</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1307923200</td>\n",
              "      <td>Cough Medicine</td>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>B006K2ZZ7K</td>\n",
              "      <td>A1UQRSCLF8GW1T</td>\n",
              "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1350777600</td>\n",
              "      <td>Great taffy</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id  ...                                               Text\n",
              "0   1  ...  I have bought several of the Vitality canned d...\n",
              "1   2  ...  Product arrived labeled as Jumbo Salted Peanut...\n",
              "2   3  ...  This is a confection that has been around a fe...\n",
              "3   4  ...  If you are looking for the secret ingredient i...\n",
              "4   5  ...  Great taffy at a great price.  There was a wid...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86Kzc7CvEznC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Task is to classify sentiments of text so keeping only the text data and rating for classification\n",
        "data = data[['Text', 'Score']].dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukds2TLIFZVu",
        "colab_type": "text"
      },
      "source": [
        "Amazon Food Review uses five-star rating system. Counting the reviews by their score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zD-8-s1FXM8",
        "colab_type": "code",
        "outputId": "da6ed154-6b6d-41da-f35c-1e32c218096a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "score_count = data['Score'].value_counts()\n",
        "plot = score_count.plot(kind='bar')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AgEvGIALa9P",
        "colab_type": "text"
      },
      "source": [
        "There are much more reviews with rating 5. To avoid using a biased dataset and to balanced it, considering rating 4 and 5 as positive class and 1, 2, and 3 as negative class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVXNmrxljyYN",
        "colab_type": "code",
        "outputId": "c00ed65f-eaec-4f87-9c39-96cdbd7b8b54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "data['Score'][data['Score'] <= 3]= str('Neg')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7iOaksjqPL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['Score'][data['Score'] != 'Neg']= str('Pos')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unpH7VwXxEev",
        "colab_type": "code",
        "outputId": "fb7f7158-29a1-4608-fffa-b69d989d09d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#checing the columns in dataset\n",
        "data.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "      <td>Pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "      <td>Neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "      <td>Pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "      <td>Neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "      <td>Pos</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text Score\n",
              "0  I have bought several of the Vitality canned d...   Pos\n",
              "1  Product arrived labeled as Jumbo Salted Peanut...   Neg\n",
              "2  This is a confection that has been around a fe...   Pos\n",
              "3  If you are looking for the secret ingredient i...   Neg\n",
              "4  Great taffy at a great price.  There was a wid...   Pos"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdJALzCTxJ-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#separating positive and negative reviews \n",
        "data_pos = data[data['Score'] == 'Pos']\n",
        "data_neg = data[data['Score'] == 'Neg']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le1hctZE5Gfp",
        "colab_type": "code",
        "outputId": "6409cad6-30c0-4364-91a6-15801894d51a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_neg.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(124677, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWcLPNDK3PDT",
        "colab_type": "code",
        "outputId": "016d0529-f73f-462c-ba80-4e2ae08a6fa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_pos.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(443777, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm2urDiIVvc0",
        "colab_type": "text"
      },
      "source": [
        "Positive review dataset has more than 2 times the reviews in negative dataset. To avoid bias, reducing the number of datapoints in positive dataset equal to the datapoints in negative dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JKlcpNh5W6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "#shuffling data to select reviews at random\n",
        "data_pos = shuffle(data_pos)\n",
        "data_pos = data_pos[:len(data_neg)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6wTgO39GVYn",
        "colab_type": "code",
        "outputId": "e0d2d7d8-2ed3-42b2-89c3-4b5b7c3a2e03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_pos.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(124677, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFc6XdUjGhPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Splitting positive class into train and test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "data_pos_train, data_pos_test = train_test_split(data_pos, test_size = 0.3, random_state = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IV6cV3pAH2Se",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Splitting negative class into train and test set\n",
        "data_neg_train, data_neg_test = train_test_split(data_neg, test_size = 0.3, random_state = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFZ6wvN6M_ks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating training dataframe\n",
        "traindf = pd.concat([data_pos_train, data_neg_train])\n",
        "traindf = shuffle(traindf)\n",
        "traindf = traindf.reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvzTK-pyPOuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating testing dataframe\n",
        "testdf = pd.concat([data_pos_test, data_neg_test])\n",
        "testdf = shuffle(testdf)\n",
        "testdf = testdf.reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkHTbveOf8DZ",
        "colab_type": "text"
      },
      "source": [
        "# 1. Tf-idf + Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBYn7biJWhTr",
        "colab_type": "text"
      },
      "source": [
        "Importing necessary libraries for TfIdf algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFFN4xPOIfvf",
        "colab_type": "code",
        "outputId": "81999946-0aed-4d3e-d11c-5beb5c20531f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer,HashingVectorizer\n",
        "#from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import string\n",
        "#import re\n",
        "import spacy\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "spacy.load('en')\n",
        "parser = English()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPJTIXAcWq8o",
        "colab_type": "text"
      },
      "source": [
        "Defining methods to be used for cleaning the text to be used for classification. Removing stopwords and punctuations. Tokenizing and Lemmatizing the words in the text to bring words having different forms but same meaning to their base forms. This is important as it helps reduce redundancy of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbIrCbUyJIaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Stop words and spcecial characters \n",
        "StopWords = set(stopwords.words('english') + list(ENGLISH_STOP_WORDS)) \n",
        "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-\", \"...\", \"”\", \"”\",\"''\"]\n",
        "\n",
        "#lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def tokenizer(text):\n",
        "  text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
        "  text = text.lower()\n",
        "  tokens = parser(text)\n",
        "  lemmas = []\n",
        "  for tok in tokens:\n",
        "      lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
        "  tokens = lemmas\n",
        "  tokens = nltk.tokenize.word_tokenize(text)\n",
        "  tokens = [tok for tok in tokens if len(tok) > 2]\n",
        "  #tokens = [lemmatizer.lemmatize(tok) for tok in tokens]\n",
        "  tokens = [tok for tok in tokens if tok not in StopWords]\n",
        "  tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
        "  tokens = list(set(tokens))\n",
        "\n",
        "  return ' '.join(tokens[:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08MzW4LGbZq6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "traindf['Text'] = traindf['Text'].apply(lambda x:tokenizer(x))\n",
        "testdf['Text'] = testdf['Text'].apply(lambda x:tokenizer(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6UXu7gIeqt5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = traindf['Score']\n",
        "x_train = traindf['Text']\n",
        "\n",
        "y_test = testdf['Score']\n",
        "x_test = testdf['Text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfx-ezlXXqgA",
        "colab_type": "text"
      },
      "source": [
        "TfidfTransformer uses CountVentorizer to count the number of words (term frequency). Training tfidf and evaluating the model on unseen data. TfidfVectorizer also works similar to tfidfTransformer although, it does certain operations all at once. The main difference between the two is that TfidfTransformer will perform step-by-step task of calculating the word count with CountVextorizer, IDF values and Tf-idf scores. Whereas, Tfidfvectorizer will do all these steps in 1 step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPYvLc5VgjR2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tfidftransformer - CountVectorizer\n",
        "count_vect = CountVectorizer(analyzer = 'word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(x_train)\n",
        "xtrain_count = count_vect.transform(x_train)\n",
        "xtest_count = count_vect.transform(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHQoSDt7_9V_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tfidfvectorizer\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
        "tfidf_vect.fit(x_train)\n",
        "xtrain_tfidf = tfidf_vect.transform(x_train)\n",
        "xtest_tfidf = tfidf_vect.transform(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIMQJp3Td0wc",
        "colab_type": "text"
      },
      "source": [
        "Using Tf-idf model with classical statistical model, Random Forest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVwhNfCtFlZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
        "    #Fitting the training dataset on the classifier\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "    \n",
        "    #Predicting the labels on validation dataset\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "    \n",
        "    return metrics.accuracy_score(predictions, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgxJzfVnFoWM",
        "colab_type": "code",
        "outputId": "68b50b26-51aa-4571-cb2b-cc8f8bb969cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn import decomposition, ensemble\n",
        "from sklearn import model_selection, preprocessing, metrics\n",
        "#RF on Count Vectors\n",
        "accuracy = training_model(ensemble.RandomForestClassifier(n_estimators=8), xtrain_count, y_train, xtest_count)\n",
        "\n",
        "print(\"RF accuracy score, Count Vectors: \", accuracy)\n",
        "\n",
        "#RF on Word Level TF IDF Vectors\n",
        "accuracy = training_model(ensemble.RandomForestClassifier(n_estimators=8), xtrain_tfidf, y_train, xtest_tfidf)\n",
        "print(\"RF accuracy score, WordLevel TF-IDF: \", accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RF accuracy score, Count Vectors:  0.8268768046198267\n",
            "RF accuracy score, WordLevel TF-IDF:  0.8361271521762378\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oU6WDOod9na",
        "colab_type": "text"
      },
      "source": [
        "The accuracy score are very close to each other for tfidfVectorizer and tfidftransformer. If computation of tfidf needs to be done on documents other than the training dataset, either of the two modules can be used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXqYwa65HBfI",
        "colab_type": "text"
      },
      "source": [
        "# 2. LSTM Text Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98Bfe47zvpwz",
        "colab_type": "code",
        "outputId": "f7f872aa-ce40-4246-df54-326b51b20f7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "#from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubBlyiS-HMY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Using the positive and negative datasets together\n",
        "\n",
        "review_text = pd.concat([data_pos['Text'], data_neg['Text']])\n",
        "review_label = pd.concat([data_pos['Score'], data_neg['Score']])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JF4NE882pS5o",
        "colab_type": "code",
        "outputId": "4132fc1f-cd08-49de-9c40-f8ecb2f25547",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Tokenizing the dataset\n",
        "#Max no. of words to use\n",
        "max_words = 50000\n",
        "#Max no. of words in each review\n",
        "max_features = 250\n",
        "#defining tokenizer\n",
        "review_text = shuffle(review_text)\n",
        "token = Tokenizer(num_words=max_words, split = ' ', filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower = True)\n",
        "token.fit_on_texts(review_text)\n",
        "word_index = token.word_index\n",
        "print(len(word_index))\n",
        "Int = token.texts_to_sequences(review_text.values)\n",
        "Int = sequence.pad_sequences(Int, maxlen=max_features)\n",
        "Int.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "92881\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(249354, 250)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_7EMCuIYZB4",
        "colab_type": "code",
        "outputId": "8d97f896-7e1d-4d94-a943-031a15396cae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "review_label = shuffle(review_label)\n",
        "Out = pd.get_dummies(review_label).values\n",
        "Out.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(249354, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R8hPfVdwagd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#splitting data\n",
        "Int_train, Int_test, Out_train, Out_test = train_test_split(Int, Out, test_size=0.2, random_state=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_B-MxB-yNo-",
        "colab_type": "code",
        "outputId": "02cd11d5-e072-48e8-e7a5-2b3157af4a80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Hyperparameters\n",
        "batch_size = 64\n",
        "embedding_dims = 100\n",
        "lstm = 128\n",
        "input_length = Int.shape[1]\n",
        "\n",
        "tf.compat.v1.get_default_graph"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function tensorflow.python.framework.ops.get_default_graph>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aG3mRr1lybw3",
        "colab_type": "code",
        "outputId": "527810a6-a124-443f-edb7-d9135c5ce3f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "#LSTM model\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dims, input_length=input_length))\n",
        "model.add(LSTM(lstm, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(2 ,activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 250, 100)          5000000   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 128)               117248    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 5,117,506\n",
            "Trainable params: 5,117,506\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYWxJMnXyozR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss\n",
        "loss = 'binary_crossentropy' \n",
        "# Optimizer\n",
        "optimizer = 'adam' \n",
        "# Compilation\n",
        "#############\n",
        "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy1c5QOQyqKC",
        "colab_type": "code",
        "outputId": "51903cd8-9385-4544-ce74-4970581c3fe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "#Training model\n",
        "model.fit(Int_train, Out_train, epochs=5, batch_size=batch_size, validation_split=0.1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "2806/2806 [==============================] - 2588s 922ms/step - loss: 0.6933 - accuracy: 0.5006 - val_loss: 0.6932 - val_accuracy: 0.4970\n",
            "Epoch 2/5\n",
            "2806/2806 [==============================] - 2570s 916ms/step - loss: 0.6909 - accuracy: 0.5283 - val_loss: 0.6958 - val_accuracy: 0.4978\n",
            "Epoch 3/5\n",
            "2806/2806 [==============================] - 2562s 913ms/step - loss: 0.6788 - accuracy: 0.5635 - val_loss: 0.7085 - val_accuracy: 0.4931\n",
            "Epoch 4/5\n",
            "2806/2806 [==============================] - 2645s 942ms/step - loss: 0.6530 - accuracy: 0.6016 - val_loss: 0.7350 - val_accuracy: 0.4958\n",
            "Epoch 5/5\n",
            "2806/2806 [==============================] - 2660s 948ms/step - loss: 0.6200 - accuracy: 0.6347 - val_loss: 0.7644 - val_accuracy: 0.4953\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe465fe2278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Micg2lUfy1TT",
        "colab_type": "text"
      },
      "source": [
        "val_loss is increasing with each incrementing epoch. The number of learning parameters can be reduced here as because of too many learning parameters, model is learning the examples and so not working well with test/validation data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5-yLTEF1Uap",
        "colab_type": "code",
        "outputId": "490df1ad-8c89-40e9-a5f5-708f685d3364",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_score = model.evaluate(Int_test, Out_test, verbose=0)\n",
        "print('Test accuracy:', test_score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.4966012239456177\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLZuWOPi0yc7",
        "colab_type": "text"
      },
      "source": [
        "There are less number of examples in the test dataset. The model performs poorly on test data as compared to training data. The model has high variance and seems to be overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9DmZfyhc1Wu",
        "colab_type": "text"
      },
      "source": [
        "#3. LSTM Model: where the embeddings are initialized with pre-trained GloVe vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns6a7IJlMIrM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating directory for storing pre-processed data\n",
        "if(not os.path.isdir('preprocessed_data')):\n",
        "    os.mkdir('preprocessed_data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PewORY7NxWiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "from keras.initializers import Constant\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SDqjdtLxkuK",
        "colab_type": "code",
        "outputId": "901448e6-acda-4af2-d278-41a01449cfcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "#Downloading pre-trained GloVe Vectors\n",
        "\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "!ls\n",
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-30 15:24:26--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-05-30 15:24:26--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-05-30 15:24:27--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1      100%[===================>] 822.24M  1.95MB/s    in 6m 27s  \n",
            "\n",
            "2020-05-30 15:30:54 (2.12 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "glove.6B.100d.txt  glove.6B.50d.txt  preprocessed_data\n",
            "glove.6B.200d.txt  glove.6B.zip      Reviews.csv\n",
            "glove.6B.300d.txt  glove.6B.zip.1    sample_data\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PebirGLl0tE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_dir = '/content/'\n",
        "glove_dir = os.path.join(base_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rCKkznj1ufW",
        "colab_type": "code",
        "outputId": "c997d60a-a19a-4655-8900-2ced5f0e9377",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Building the index mapping words in the embeddings set to their respective embedding vector\n",
        "\n",
        "embedding_index = {}\n",
        "with open(os.path.join(glove_dir, 'glove.6B.300d.txt')) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
        "        embedding_index[word] = coefs\n",
        "\n",
        "print('Found %s word vectors.' % len(embedding_index))\n",
        "#print(len(embedding_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm4eYsJ6P5jo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQ_LENGTH = 50 #max length of each sentence\n",
        "MAX_WORDS = 150000 #max no. of words for tokenizer\n",
        "EMBEDDING_DIM = 300 #wmbedding dimensions for word vectors (GloVe)\n",
        "LSTM_SIZE = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF665IzF1ymW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Tokenizing the dataset\n",
        "max_features = 100\n",
        "tokenizer = Tokenizer(num_words=max_features, split=' ', filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "tokenizer.fit_on_texts(review_text)\n",
        "X_gl = tokenizer.texts_to_sequences(review_text.values)\n",
        "X_gl = sequence.pad_sequences(X_gl, maxlen=max_features)\n",
        "review_label = shuffle(review_label)\n",
        "Y_gl = pd.get_dummies(review_label).values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SPzvJhU13b2",
        "colab_type": "code",
        "outputId": "6a3b7692-c51a-47bf-d589-a7918040c517",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Vectorizing the text samples into a 2D integer tensor\n",
        "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
        "tokenizer.fit_on_texts(review_text)\n",
        "sequences = tokenizer.texts_to_sequences(review_text.values)\n",
        "\n",
        "word_idx = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_idx))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQ_LENGTH)\n",
        "\n",
        "#labels = to_categorical(np.asarray(review_label))\n",
        "labels = np.asarray(pd.get_dummies(review_label))\n",
        "print('Data tensor shape:', data.shape)\n",
        "print('Label tensor shape:', labels.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 93000 unique tokens.\n",
            "Data tensor shape: (249354, 50)\n",
            "Label tensor shape: (249354, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80iXNd8V0w2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating a weight matrix\n",
        "num_words = min(MAX_WORDS, len(word_idx) + 1)\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "for word, i in word_idx.items():\n",
        "    if i >= MAX_WORDS:\n",
        "        continue\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS5WxI3NKEi2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data split into training  and test set \n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "num_validation_samples = int(0.2 * data.shape[0])\n",
        "\n",
        "x_train_gl = data[:-num_validation_samples]\n",
        "y_train_gl = labels[:-num_validation_samples]\n",
        "x_test_gl = data[-num_validation_samples:]\n",
        "y_test_gl = labels[-num_validation_samples:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHEI-a-5Rs2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Embedding Layer\n",
        "embedding_layer = Embedding(input_dim=len(word_idx) + 1,\n",
        "                                output_dim=EMBEDDING_DIM,\n",
        "                                weights=[embedding_matrix],\n",
        "                                input_length=MAX_SEQ_LENGTH,\n",
        "                                trainable=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPMn_V_uR2gL",
        "colab_type": "code",
        "outputId": "8621df21-910c-42c8-e4d2-c2cd2b86292b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(LSTM(LSTM_SIZE, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 50, 300)           27900300  \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 300)               721200    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 602       \n",
            "=================================================================\n",
            "Total params: 28,622,102\n",
            "Trainable params: 28,622,102\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0y7-za1R5BS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loss\n",
        "loss = 'binary_crossentropy' \n",
        "#Optimizer\n",
        "optimizer = 'nadam' \n",
        "#Model Compilation\n",
        "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gy48-QP6R7eb",
        "colab_type": "code",
        "outputId": "02b4b494-ed9e-499b-bcc7-6b070c6d00dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "#Fitting model\n",
        "model.fit(x_train_gl, y_train_gl,validation_split=0.2,batch_size=120,epochs=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 159587 samples, validate on 39897 samples\n",
            "Epoch 1/5\n",
            "159587/159587 [==============================] - 391s 2ms/sample - loss: 0.6953 - accuracy: 0.5007 - val_loss: 0.6936 - val_accuracy: 0.5006\n",
            "Epoch 2/5\n",
            "159587/159587 [==============================] - 385s 2ms/sample - loss: 0.6916 - accuracy: 0.5179 - val_loss: 0.6948 - val_accuracy: 0.5036\n",
            "Epoch 3/5\n",
            "159587/159587 [==============================] - 386s 2ms/sample - loss: 0.6828 - accuracy: 0.5477 - val_loss: 0.6999 - val_accuracy: 0.5011\n",
            "Epoch 4/5\n",
            "159587/159587 [==============================] - 384s 2ms/sample - loss: 0.6628 - accuracy: 0.5804 - val_loss: 0.7183 - val_accuracy: 0.4987\n",
            "Epoch 5/5\n",
            "159587/159587 [==============================] - 383s 2ms/sample - loss: 0.6347 - accuracy: 0.6144 - val_loss: 0.7519 - val_accuracy: 0.5010\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9c04312550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4aumApE33Ii",
        "colab_type": "text"
      },
      "source": [
        "There is a slight improvement in model performance with pretrained GloVe vectors with introduction of weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obI5aVUzSEu5",
        "colab_type": "code",
        "outputId": "5da9b4af-b2d4-465f-e36d-c9c35acaffb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "scores = model.evaluate(x_test_gl, y_test_gl, verbose=0)\n",
        "print('Test Accuracy:' , scores[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.50382996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnmSCE0D3zZy",
        "colab_type": "text"
      },
      "source": [
        "There are less number of examples in the test dataset. The model performs poorly on test data as compared to training data. The model has high variance and seems to be overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXjcuN9-SbPt",
        "colab_type": "text"
      },
      "source": [
        "# 4. fastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUu3X894DYqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install ktrain"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DEZUJ9qDdFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install fasttext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ng4AeNMFDgB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ktrain\n",
        "from ktrain import text\n",
        "#%tensorflow_version 2.x\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWXf_0fUaCdy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "df = pd.read_csv('/content/Reviews.csv', sep = ',', engine='python')\n",
        "df = shuffle(df)\n",
        "df = df[:75000]\n",
        "df = df[['Text', 'Score']]\n",
        "df['Score'] = df['Score'].apply(lambda x: 'neg' if x <= 3 else 'pos')\n",
        "df.columns = ['text', 'label']\n",
        "df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br7-GV_q-gan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.concat([df, df.label.astype('str').str.get_dummies()], axis=1, sort=False)\n",
        "df.head(5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxbwOZvp-pqA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.drop(columns='label')\n",
        "df = df[['text', 'neg', 'pos']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OezPyfGgE3L2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test), preproc = text.texts_from_df(df, 'text', label_columns=['neg', 'pos'],\n",
        "                                                                  maxlen=75, max_features=100000,\n",
        "                                                                  preprocess_mode='standard', val_pct=0.1,\n",
        "                                                                  ngram_range=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQze2oAscX7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text.print_text_classifiers()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrTWX1RGcczY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = text.text_classifier('fasttext', (x_train, y_train), preproc=preproc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o9opbGvcjAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner_mod = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNOPuu-DczHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Learning rate finder\n",
        "learner_mod.lr_find()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uKzjPxXc089",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Learning rate plot\n",
        "learner_mod.lr_plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW2ILsJcc2yl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner_mod.autofit(0.0009, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XujMix8nc5Xs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicting = ktrain.get_predictor(learner_mod.model, preproc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZILVpsy0c8Nv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = [ 'It was a boring movie! What a waste of money.',\n",
        "         'What an amazing piece of art. I would see the movie again.',\n",
        "        'What a beautiful romantic comedy.']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkH5WUh2c-vN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicting.predict(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVj09Bl6lP5t",
        "colab_type": "text"
      },
      "source": [
        "# 5. BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeSO3pdFlY5C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test), preproc = text.texts_from_df(df, 'text', label_columns=['neg', 'pos'],\n",
        "                                                                      maxlen = 550,\n",
        "                                                                      preprocess_mode='bert')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qg2nsDjmE1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner_ktrain = ktrain.get_learner(text.text_classifier('bert',(x_train,y_train),preproc=preproc),\n",
        "                                                train_data=(x_train,y_train),\n",
        "                                                val_data=(x_test, y_test),\n",
        "                                                batch_size=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mr9ogsacmILy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner_ktrain.lr_find()\n",
        "learner_ktrain.lr_plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sfjskl9xmKNY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner_ktrain.fit_onecycle(2e-5, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_bVroWufomi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicting = ktrain.get_predictor(learner_mod.model, preproc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2AJznJWfslu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = [ 'It was a boring movie! What a waste of money.',\n",
        "         'What an amazing piece of art. I would see the movie again.',\n",
        "        'What a beautiful romantic comedy.']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1j-0gZbTfw5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicting.predict(data)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}